# - Run training with `python -m cs336_basics.train --config default.yml`
# - Run from project root (paths are relative to `cwd`)
# - Will always eval and save checkpoint at the end of training

run:
  run_id: "l-run-<timestamp>"
  out_dir: "/data/c-sniderb/runs"
  # wandb_project: false
  wandb_project: "cs336-a1"
  wandb_tags: ["ts-exp"]

data:
  train_data_path: "/data/c-sniderb/tokens/ts-train/tokens.bin"
  valid_data_path: "/data/c-sniderb/tokens/ts-valid/tokens.bin"

  # d_model | d_ff (8/3) | d_ff (4/3)
  # 768     | 2048       | 3072
  # 1024    | 2688       | 4096
  # 1280    | 3456       | 5120
  # 1536    | 4906       | 6144
  # 1792    | 4736       | 7168
  # 2048    | 6144       | 8192

model:
  d_model: 512
  d_ff: 1344
  num_heads: 16
  num_layers: 4
  vocab_size: 10_000
  context_length: 256
  rope_theta: 10_000
  ffn_type: "swiglu"

  zero_init_attn_out: true
  zero_init_ffn_out: true
  zero_init_lm_head: false

  # Use both or neither
  tie_weights: true
  tied_weights_std: 0.02


optimizer:
  # Example AdamW config
  name: "adamw" # "adamw", "muon"
  lr: 6.0e-4
  betas: [0.9, 0.95]
  eps: 1.0e-8
  weight_decay: 0.01

  # Here to make sure defaults are set
  momentum: false
  nesterov: false
  ns_steps: false
  adamw_betas: false
  adamw_eps: false

  # Example Muon config (note that AdamW and Muon configs are mutually exclusive)
  # - Also note: AdamW still used for some params, so those opts can be configured
  # name: "muon"
  # lr: 6.0e-4
  # weight_decay: 0.01
  # momentum: 0.95
  # nesterov: true
  # ns_steps: 5
  # adamw_betas: [0.9, 0.95]
  # adamw_eps: 1.0e-8

# (batch_size * max_steps * context_length) ~= 327,680,000 (327.68M)
# e.g. 32 * 40_000 * 256 = 327,680,000
# e.g. 64 * 20_000 * 256 = 327,680,000
# e.g. 128 * 10_000 * 256 = 327,680,000

training:
  grad_accum_steps: 1
  batch_size: 128
  max_steps: 16_000
  eval_before_training: false
  eval_interval: 200
  eval_steps: 2
  eval_batch_size: 128
  checkpoint_interval: 30_000
  max_l2_norm: 1.0

  # Cosine LR schedule
  lr_schedule: "linear" # "cosine", "linear", "double"
  lr_max: 6.0e-4
  lr_min: 1.0e-4
  warmup_ratio: 0.003 # Only used if `warmup_iters` is not set
  # cosine_cycle_iters: 16_000

  # Only used with other LR schedules (here to make sure defaults are set)
  lr_inter: False # unused with cosine schedule
  warmup_iters: False # overrides `warmup_ratio` if set in inheriting config
  cosine_cycle_iters: False
  linear_cycle_iters: False
  exp_decay_iters: False
  phase_two_iters: False
  phase_two_type: "linear"

  # Example of how to configure Linear LR schedule
  # lr_schedule: "linear"
  # lr_max: 5.0e-4
  # lr_min: 1.8e-4
  # warmup_ratio: 0.03
  # linear_cycle_iters: 13_000

  # Example of how to configure Double LR schedule
  # lr_schedule: "double"
  # lr_max: 1.5e-3
  # lr_inter: 6.0e-4
  # lr_min: 2.0e-4
  # warmup_iters: 200
  # exp_decay_iters: 1_500 # iter at which to hit lr_inter
  # phase_two_iters: 13_000 # iter at which to hit lr_min
  # phase_two_type: "linear"

  # device: "cuda:7" # prefer setting device in command line
