# - Train: `uv run -m leaderboard.train --config leaderboard/configs/base.yml`
# - Will always eval and save checkpoint at the end of training

run:
  run_id: "<optimizer>-lr<lr_max>-d<d_model>f_<d_ff>_h<num_heads>_l<num_layers>-<max_steps>-<timestamp>"
  out_dir: "/data/c-sniderb/runs"
  # wandb_project: false
  wandb_project: "cs336-a1"
  wandb_tags: ["owt","leaderboard"]

data:
  train_data_path: "/data/c-sniderb/tokens/owt-train/tokens.bin"
  valid_data_path: "/data/c-sniderb/tokens/owt-valid/tokens.bin"

  # d_model | d_ff (8/3) | d_ff (4/3)
  # 768     | 2048       | 3072
  # 1024    | 2688       | 4096
  # 1280    | 3456       | 5120
  # 1536    | 4906       | 6144
  # 1792    | 4736       | 7168
  # 2048    | 6144       | 8192

model:
  d_model: 1280
  d_ff: 3456
  num_heads: 16
  num_layers: 12
  vocab_size: 32_000
  rope_theta: 10_000
  ffn_type: "swiglu"
  context_length: 512
  ln_after_embed: true
  clamp_logits_value: 15
  use_value_residual: false

  zero_init_attn_out: true
  zero_init_ffn_out: true
  # zero_init_lm_head: true

  use_embed_shortcut: false

  # Use both or neither
  tie_weights: true
  tied_weights_std: 0.02


optimizer:
  # Just to make sure keys are present (set name to "adamw" )
  # - To use AdammW, set `name: "adamw"` and set betas and eps in an inheriting config
  betas: False
  eps: False

  # Example AdamW config
  name: "muon" # "adamw", "muon"
  weight_decay: 0.01
  momentum_start: 0.95
  momentum: 0.95
  momentum_warmup_steps: 0
  nesterov: true
  ns_steps: 5
  adamw_betas: [0.9, 0.95]
  adamw_eps: 1.0e-8
  use_adamw_groups: false

training:
  grad_accum_steps: 1
  batch_size: 128
  max_steps: 12_000
  eval_before_training: true
  eval_interval: 100
  eval_steps: 10 # Only used <500 steps from end (1 until then)
  eval_batch_size: 128
  checkpoint_interval: 10_000
  max_l2_norm: 1.0

  # Optionally use AdamW optimizer groups (with Muon for hidden params)
  # - Note: should unset `tie_weights` if using this
  lr_embed: 1.6e-2
  lr_lm_head: 1.6e-4
  lr_scalar: 1.6e-3

  # Usage of LR scheduler (lr_max applies to Muon if `use_adamw_groups` is true):
  lr_max: 1.6e-3
  warmup_ratio: 0
  warmup_iters: False # or warmup_ratio
  lr_decay_schedule:
    - until_iter: 11_300
      to_lr: 1.6e-4
      type: "linear"
    - until_iter: 12_000
      to_lr: 0
      type: "linear"

  # Optionally define custom sequence length schedule
  seq_len_min: 512
  seq_len_schedule:
    - until_iter: 1_000_000
      to_seq_len: 512

  # Optionally define custom batch size schedule
  batch_size_max: 128
  batch_size_schedule:
    - until_iter: 1_000_000
      to_batch_size: 128
