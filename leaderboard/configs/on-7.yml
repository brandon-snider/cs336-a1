model:
  d_model: 1280
  d_ff: 3456
  num_heads: 16
  num_layers: 12

optimizer:
  name: "muon"
  weight_decay: 0.01
  momentum: 0.95
  nesterov: true
  ns_steps: 5
  adamw_betas: [0.9, 0.95]
  adamw_eps: 1.0e-8

training:
  max_steps: 12_000
  
  # Save a checkpoint before messing with LR schedule
  checkpoint_interval: 11_500
  eval_interval: 100
  eval_steps: 1

  # LR schedule with custom scheduler
  lr_max: 1.6e-3
  warmup_ratio: 0
  warmup_iters: False # or warmup_ratio
  lr_decay_schedule:
    - until_iter: 11_500
      to_lr: 1.6e-4
      type: "linear"
    - until_iter: 12_000
      to_lr: 0
      type: "linear"