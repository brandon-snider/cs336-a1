run:
  wandb_project: false
  wandb_tags: ["owt", "leaderboard"]

data:
  train_data_path: "/data/c-sniderb/tokens/owt-train/tokens.bin"
  valid_data_path: "/data/c-sniderb/tokens/owt-valid/tokens.bin"

model:
  d_model: 768
  d_ff: 2048
  # d_ff: 4096
  num_heads: 12
  num_layers: 10
  context_length: 512
  vocab_size: 32_000
  weight_tying: true
  ffn_type: "swiglu"
  # ffn_type: "silu"
  attn_impl: "normal" 
  embedding_std: 0.02

optimizer:
  lr: 1.0e-3
  betas: [0.9, 0.95]
  eps: 1.0e-8
  weight_decay: 0.1

# Total tokens = batch_size * max_steps * context_length
# 256 * 22_000 * 256 = 1,441,792,000 = 1.442B
# Required throughput = 1.442B tokens / 5400s = 267k tokens/s
# Current empirical throughput: 268k tokens/s

training:
  batch_size: 192
  max_steps: 25_000
  eval_interval: 4_000
  eval_steps: 15
  eval_batch_size: 256
  checkpoint_interval: 5_000
  max_l2_norm: 1.0
  lr_max: 1.0e-3
  lr_min: 1.0e-4
  warmup_ratio: 0.01