# - Run training with `python -m cs336_basics.train --config default.yml`
# - Run from project root (paths are relative to `cwd`)
# - Will always eval and save checkpoint at the end of training

run:
  run_id: "run-<timestamp>"
  # out_dir: "./out/runs"
  out_dir: "/data/c-sniderb/runs"
  wandb_project: "cs336-a1"
  # wandb_project: false
  wandb_tags: []

data:
  # train_data_path: "./out/tokens/ts-train/tokens.bin"
  # valid_data_path: "./out/tokens/ts-valid/tokens.bin"
  train_data_path: "/data/c-sniderb/tokens/ts-train/tokens.bin"
  valid_data_path: "/data/c-sniderb/tokens/ts-valid/tokens.bin"

model:
  d_model: 512
  num_heads: 16
  d_ff: 1344
  vocab_size: 10_000
  context_length: 256
  num_layers: 4
  rope_theta: 10_000
  attn_impl: "normal" # "normal", "chunked"
  # attn_chunk_size: 1024


optimizer:
  lr: 6.0e-3
  betas: [0.9, 0.999]
  eps: 1.0e-8
  weight_decay: 0.01

# (batch_size * max_steps * context_length) ~= 327,680,000 (327.68M)
# e.g. 32 * 40_000 * 256 = 327,680,000
# e.g. 64 * 20_000 * 256 = 327,680,000
# e.g. 128 * 10_000 * 256 = 327,680,000
training:
  batch_size: 128
  max_steps: 10_000
  eval_before_training: false
  eval_interval: 1000
  eval_steps: 50
  eval_batch_size: 128
  checkpoint_interval: 10_000
  max_l2_norm: 1.0
  lr_max: 6.0e-3
  lr_min: 0
  warmup_ratio: 0.05
  lr_schedule: "cosine"
